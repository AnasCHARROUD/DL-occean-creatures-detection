{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fiftyone\n"
      ],
      "metadata": {
        "id": "nSKhUgnDNHhl"
      },
      "id": "nSKhUgnDNHhl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python-headless==4.5.2.52"
      ],
      "metadata": {
        "id": "UJWlPx9zBqe0"
      },
      "id": "UJWlPx9zBqe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkNmBTnJB9Zn",
        "outputId": "c0116423-e76d-4687-bdd2-d44165b4f53e"
      },
      "id": "PkNmBTnJB9Zn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumExpr defaulting to 2 threads.\n",
            "Migrating database to v0.15.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## This to download data from openimage ----trainning don't run it ###############\n",
        "\n",
        "dataset = fo.zoo.load_zoo_dataset(\n",
        "              \"open-images-v6\",\n",
        "              split=\"train\",\n",
        "              label_types=[\"detections\"],\n",
        "              classes=['Jellyfish', 'Dolphin', 'Starfish', 'Shellfish', 'Turtle']\n",
        "          )\n"
      ],
      "metadata": {
        "id": "QzKvOONoQzOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc11d75-e79e-404d-d67e-abc4418c9f37"
      },
      "id": "QzKvOONoQzOR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/open-images-v6/train' if necessary\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to '/root/fiftyone/open-images-v6/train/metadata/image_ids.csv'\n",
            " 100% |██████|    4.8Gb/4.8Gb [4.0s elapsed, 0s remaining, 1.2Gb/s]       \n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v6/train/metadata/classes.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmp6bdep79l/metadata/hierarchy.json'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to '/root/fiftyone/open-images-v6/train/labels/detections.csv'\n",
            "Downloading 2573 images\n",
            " 100% |█████████████████| 2573/2573 [3.3m elapsed, 0s remaining, 12.2 files/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'train'\n",
            " 100% |███████████████| 2573/2573 [16.6s elapsed, 0s remaining, 143.3 samples/s]      \n",
            "Dataset 'open-images-v6-train' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## This to download data from openimage ----testing don't run it ###############\n",
        "\n",
        "dataset = fo.zoo.load_zoo_dataset(\n",
        "              \"open-images-v6\",\n",
        "              split=\"test\",\n",
        "              label_types=[\"detections\"],\n",
        "              classes=['Jellyfish', 'Dolphin', 'Starfish', 'Shellfish', 'Turtle']\n",
        "          )\n"
      ],
      "metadata": {
        "id": "gPsmd2gChisJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109d0847-4366-430b-9c41-d66c9affc7c8"
      },
      "id": "gPsmd2gChisJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'test' to '/root/fiftyone/open-images-v6/test' if necessary\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to '/root/fiftyone/open-images-v6/test/metadata/image_ids.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v6/test/metadata/classes.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmpvl9xqw43/metadata/hierarchy.json'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to '/root/fiftyone/open-images-v6/test/labels/detections.csv'\n",
            "Downloading 469 images\n",
            " 100% |███████████████████| 469/469 [37.3s elapsed, 0s remaining, 13.1 files/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'test'\n",
            " 100% |█████████████████| 469/469 [3.9s elapsed, 0s remaining, 127.8 samples/s]      \n",
            "Dataset 'open-images-v6-test' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## This to download data from openimage ----validation don't run it ###############\n",
        "dataset = fo.zoo.load_zoo_dataset(\n",
        "              \"open-images-v6\",\n",
        "              split=\"validation\",\n",
        "              label_types=[\"detections\"],\n",
        "              classes=['Jellyfish', 'Dolphin', 'Starfish', 'Shellfish', 'Turtle']\n",
        "          )\n"
      ],
      "metadata": {
        "id": "I7kQtHLNhzk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809c632e-b4fa-4de7-f13a-195b20cf11e3"
      },
      "id": "I7kQtHLNhzk6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/open-images-v6/validation' if necessary\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv' to '/root/fiftyone/open-images-v6/validation/metadata/image_ids.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v6/validation/metadata/classes.csv'\n",
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmp8361_ppa/metadata/hierarchy.json'\n",
            "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv' to '/root/fiftyone/open-images-v6/validation/labels/detections.csv'\n",
            "Downloading 172 images\n",
            " 100% |███████████████████| 172/172 [13.5s elapsed, 0s remaining, 14.3 files/s]      \n",
            "Dataset info written to '/root/fiftyone/open-images-v6/info.json'\n",
            "Loading 'open-images-v6' split 'validation'\n",
            " 100% |█████████████████| 172/172 [1.4s elapsed, 0s remaining, 123.5 samples/s]         \n",
            "Dataset 'open-images-v6-validation' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLk5OSTF6Peq",
        "outputId": "cec02f7e-3b54-48f8-8516-67f7e398807c"
      },
      "id": "tLk5OSTF6Peq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "uqfjAz69ANyW"
      },
      "id": "uqfjAz69ANyW"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chitra"
      ],
      "metadata": {
        "id": "TbYthNK1CHCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71400bcd-7c83-44ee-985a-830c505a82bc"
      },
      "id": "TbYthNK1CHCp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chitra in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from chitra) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from chitra) (3.2.2)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.7/dist-packages (from chitra) (0.4.1)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from chitra) (2.7.1)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from chitra) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (1.21.5)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (1.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (4.1.2.30)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (2.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->chitra) (1.8.1.post1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->chitra) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->chitra) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->chitra) (2021.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->chitra) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->chitra) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->chitra) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->chitra) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->chitra) (3.10.0.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer->chitra) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c67de1",
      "metadata": {
        "id": "b4c67de1"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "from chitra.image import Chitra\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import xml.etree.ElementTree as ET\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.retinanet import RetinaNet\n",
        "import warnings\n",
        "import transforms as T\n",
        "from torchvision.transforms import transforms as T1\n",
        "from torchvision.transforms import AutoAugment\n",
        "from torchvision.transforms import functional as TF\n",
        "from torchvision.transforms.autoaugment import AutoAugmentPolicy\n",
        "import utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from engine import train_one_epoch\n",
        "from engine import evaluate\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchvision -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWu4usKLJHMB",
        "outputId": "cb4bb26e-5b8a-4c69-c8db-b0c2c00cbeaa"
      },
      "id": "kWu4usKLJHMB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faa902e4",
      "metadata": {
        "id": "faa902e4"
      },
      "outputs": [],
      "source": [
        "def create_corner_rect(bb, color='white'):\n",
        "    bb = np.array(bb.detach(), dtype=np.float64)\n",
        "    return plt.Rectangle((bb[0], bb[1]),bb[2]-bb[0], bb[3]-bb[1] , color=color,\n",
        "                         fill=False, lw=3)\n",
        "\n",
        "def show_corner_bb(im, bb, color):\n",
        "    plt.imshow(im)\n",
        "    plt.gca().add_patch(create_corner_rect(bb,color))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc3139d",
      "metadata": {
        "id": "7dc3139d"
      },
      "outputs": [],
      "source": [
        "data_dir = os.listdir(\"/content/drive/MyDrive/data/openimages_dataset\")\n",
        "clase = ['Jellyfish', 'Fish', 'Dolphin', 'Starfish', 'Shellfish', 'Turtle']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b001d36",
      "metadata": {
        "id": "4b001d36"
      },
      "outputs": [],
      "source": [
        "Train = pd.read_csv(\"/content/drive/MyDrive/data/openimages_dataset/train/labels/detections.csv\", sep=\",\")\n",
        "Train_classes = pd.read_csv(\"/content/drive/MyDrive/data/openimages_dataset/train/metadata/classes.csv\", sep=\",\")\n",
        "Train_classes.columns = ['classID', 'classname']\n",
        "Train_classes = Train_classes[Train_classes['classname'].isin(clase)]\n",
        "Train = Train[Train['LabelName'].isin(Train_classes['classID'])]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test = pd.read_csv(\"/content/drive/MyDrive/data/openimages_dataset/test/labels/detections.csv\", sep=\",\")\n",
        "Test_classes = pd.read_csv(\"/content/drive/MyDrive/data/openimages_dataset/test/metadata/classes.csv\", sep=\",\")\n",
        "Test_classes.columns = ['classID', 'classname']\n",
        "Test_classes = Test_classes[Test_classes['classname'].isin(clase)]\n",
        "Test = Test[Test['LabelName'].isin(Test_classes['classID'])]\n",
        "\n",
        "\n",
        "Valid  = pd.read_csv(\"/content/drive/MyDrive/data/openimages_dataset/validation/labels/detections.csv\", sep=\",\")\n",
        "Valid_classes = pd.read_csv(\"/content/drive/MyDrive/data/openimages_dataset/validation/metadata/classes.csv\", sep=\",\")\n",
        "Valid_classes.columns = ['classID', 'classname']\n",
        "Valid_classes = Valid_classes[Valid_classes['classname'].isin(clase)]\n",
        "Valid = Valid[Valid['LabelName'].isin(Valid_classes['classID'])]"
      ],
      "metadata": {
        "id": "KuFrx-CE8Z0o"
      },
      "id": "KuFrx-CE8Z0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac502ac6",
      "metadata": {
        "id": "ac502ac6"
      },
      "outputs": [],
      "source": [
        "def get_transform1(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomPhotometricDistort())\n",
        "        transforms.append(T.RandomZoomOut())\n",
        "        transforms.append(T.RandomIoUCrop())\n",
        "        transforms.append(T.ScaleJitter((224,224)))\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347ce647",
      "metadata": {
        "id": "347ce647"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FothomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms, Train, Train_classes):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.images_names = os.listdir(self.root)\n",
        "        self.Train = Train\n",
        "        self.Train_classes =Train_classes\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and bounding boxes\n",
        "        # 1st load image\n",
        "        img_path = self.images_names[idx]\n",
        "        img = Image.open(os.path.join(self.root,img_path)).convert(\"RGB\")\n",
        "        width, height = img.size\n",
        "\n",
        "        # 2end extract bounding boxes\n",
        "        example_of_image = self.Train[self.Train['ImageID']==img_path[:-4]]\n",
        "        boxes = np.array(example_of_image[['XMin','XMax','YMin','YMax']])\n",
        "        boxes[:,0] = boxes[:,0]*width\n",
        "        boxes[:,1] = boxes[:,1]*width\n",
        "        boxes[:,2] = boxes[:,2]*height\n",
        "        boxes[:,3] = boxes[:,3]*height\n",
        "        boxes =boxes[:,[0,2,1,3]]\n",
        "        boxes = boxes.tolist()\n",
        "        # 3erd extract labels\n",
        "        labels = []\n",
        "        for e in range(len(example_of_image)):\n",
        "          labels.append(clase.index(self.Train_classes[example_of_image['LabelName'].iloc[e]==self.Train_classes['classID']]['classname'].iloc[0]))\n",
        "        \n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        boxe = []\n",
        "        for k,elem in enumerate(boxes): \n",
        "          resize = Chitra(img,elem,labels[k] )\n",
        "          resize.resize_image_with_bbox((224, 224))\n",
        "          if(k == len(boxes)-1):\n",
        "            img = resize.image\n",
        "          boxes1 = np.concatenate(resize.bboxes[0][:], axis=None)\n",
        "          boxe.append(list(boxes1))\n",
        "        boxes = torch.as_tensor(boxe, dtype=torch.float32)\n",
        "        '''print(img)\n",
        "        print(boxes)\n",
        "        print(labels[0])'''\n",
        "\n",
        "\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        #target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        y= []\n",
        "        y.append(target)\n",
        "        #bg = T1.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "        #img = bg(img)\n",
        "        return img, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_names)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcb4a339",
      "metadata": {
        "id": "fcb4a339"
      },
      "outputs": [],
      "source": [
        "def model_choice(modeltype,num_classes): #choose between the exist model in pytorch modeltype: ['fasterrcnn, mobilenet']\n",
        "    if(modeltype == 'model_0'):\n",
        "        model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "\n",
        "    if(modeltype == 'model_1'):\n",
        "        model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "    \n",
        "    if(modeltype == 'model_2'):\n",
        "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    if(modeltype == 'model_3'):\n",
        "        model = torchvision.models.detection.FasterRCNN(pretrained=True)\n",
        "\n",
        "    num_classes = num_classes  \n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd1ec8c",
      "metadata": {
        "id": "acd1ec8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e168cfee-aad5-4918-f2f9-444d3e47d0a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7198e5510cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# get the model using our helper function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# move model to the right device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8ba4528a044f>\u001b[0m in \u001b[0;36mmodel_choice\u001b[0;34m(modeltype, num_classes)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeltype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'model_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFasterRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'pretrained'"
          ]
        }
      ],
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has 5 classes \n",
        "num_classes = len(clase)\n",
        "# use our dataset and defined transformations (our class)\n",
        "dataset =  FothomDataset('/content/drive/MyDrive/data/openimages_dataset/train/data', get_transform1(train=True),Train,Train_classes)\n",
        "dataset_valid =  FothomDataset('/content/drive/MyDrive/data/openimages_dataset/validation/data', get_transform1(train=False),Valid,Valid_classes)\n",
        "\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-4000])\n",
        "\n",
        "# this function from torch read and prepare the data to be iterable, means data_loader contain, at each iteration, the image and the bb and the labels\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=1)\n",
        "\n",
        "data_loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1)\n",
        "\n",
        "# get the model using our helper function \n",
        "model = model_choice('model_0',num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=1e-3,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78f4f89",
      "metadata": {
        "id": "c78f4f89"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_valid, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583888f7",
      "metadata": {
        "id": "583888f7"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "torch.save(checkpoint, 'checkpoint_frcnnm_bn.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model1 = model_choice('model_0',num_classes)\n",
        "model1 = model1.to(device)\n",
        "checkpoint = torch.load('checkpoint_frcnnm_bn.pth')\n",
        "model1.load_state_dict(checkpoint['state_dict'])\n",
        "model1.eval()"
      ],
      "metadata": {
        "id": "nV5kXyNWan70"
      },
      "id": "nV5kXyNWan70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test **"
      ],
      "metadata": {
        "id": "StsvG7qDVYPZ"
      },
      "id": "StsvG7qDVYPZ"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test =  FothomDataset('/content/drive/MyDrive/data/openimages_dataset/test/data', get_transform1(train=False),Test,Test_classes)\n",
        "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1)"
      ],
      "metadata": {
        "id": "MGNQWxbxM1Oq"
      },
      "id": "MGNQWxbxM1Oq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model1, data_loader_test, device)"
      ],
      "metadata": {
        "id": "sgkWPYx_XkS1"
      },
      "id": "sgkWPYx_XkS1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate accuracy"
      ],
      "metadata": {
        "id": "XxDHmUFO30Cz"
      },
      "id": "XxDHmUFO30Cz"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def binary_acc(predicted_label, correct_lablel):\n",
        "    return (sum([1 for i, j in zip(predicted_label, correct_lablel) if i == j[0]])/len(predicted_label))*100\n",
        "\n",
        "def val_metrics(model, valid_dl, C=100):\n",
        "    model.eval()\n",
        "    correct_lablel=[]\n",
        "    predicted_label =[]\n",
        "    i = 0\n",
        "    for images, targets in valid_dl:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        boxes =targets[0]['boxes']\n",
        "        y_class = targets[0]['labels']\n",
        "        if(len(boxes==3)):\n",
        "            #print(boxes)\n",
        "            element = max(boxes.shape[0],boxes.shape[1])\n",
        "            boxes = torch.reshape(boxes, (element,boxes.shape[2]))   \n",
        "            targets[0]['boxes']= boxes\n",
        "        output = model(images)\n",
        "        out_bb = output[0]['boxes']\n",
        "        out_class = output[0]['labels']\n",
        "        correct_lablel.append(y_class[0])\n",
        "        try :\n",
        "            predicted_label.append(out_class[0])\n",
        "        except :\n",
        "            predicted_label.append(0)\n",
        "        \n",
        "    return predicted_label,correct_lablel"
      ],
      "metadata": {
        "id": "tGXLvWg3NU8N"
      },
      "id": "tGXLvWg3NU8N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label,correct_lablel = val_metrics(model1, data_loader_test, C=1000)"
      ],
      "metadata": {
        "id": "qMha92NV480r"
      },
      "id": "qMha92NV480r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_acc(predicted_label, correct_lablel)"
      ],
      "metadata": {
        "id": "m6PkKWWb5Zco"
      },
      "id": "m6PkKWWb5Zco",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label1,correct_lablel1 = val_metrics(model1, data_loader_valid, C=1000)"
      ],
      "metadata": {
        "id": "KLr2_6XA8olM"
      },
      "id": "KLr2_6XA8olM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_acc(predicted_label1, correct_lablel1)"
      ],
      "metadata": {
        "id": "3Vjm4r9a8sPo"
      },
      "id": "3Vjm4r9a8sPo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#take some image for test"
      ],
      "metadata": {
        "id": "DTjIc87f7-Xm"
      },
      "id": "DTjIc87f7-Xm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe156f02",
      "metadata": {
        "id": "fe156f02"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "labelsl = clase\n",
        "    \n",
        "# choose randomly five image to test our mpdel visualy\n",
        "data_rot = '/content/drive/MyDrive/data/openimages_dataset/test/data'\n",
        "image_name = os.listdir(data_rot) \n",
        "# pick randomly five image name from the test folder\n",
        "test_image = random.sample(image_name,5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose a number between 0 to 4\n",
        "b = 4 \n",
        "\n",
        "\n",
        "#this calculate the image classes\n",
        "nnn = Test[Test['ImageID']==test_image[b][:-4]]['LabelName']\n",
        "images_classe = []\n",
        "for ele in nnn:\n",
        "  images_classe.append(Test_classes[Test_classes['classID']==ele]['classname'].iloc[0])\n",
        "img = Image.open(os.path.join(data_rot,test_image[b])).convert(\"RGB\")\n",
        "img = img.resize((224,224))\n",
        "transformTotensor = T1.Compose([T1.ToTensor()])\n",
        "img1 = transformTotensor(img)\n",
        "img1 = img1.unsqueeze(0)\n",
        "img1 = img1.to(device)\n",
        "\n",
        "loss = model1(img1)\n",
        "\n",
        "try :\n",
        "  labelssss = loss[0]['labels'][0]\n",
        "  \n",
        "except:\n",
        "  labelssss = 0\n",
        "\n",
        "image = Chitra(img, bboxes=loss[0]['boxes'][0].cpu().detach().numpy(), labels=labelsl[labelssss])\n",
        "plt.imshow(image.draw_boxes())\n",
        "\n",
        "print('the labels provided by our model are')\n",
        "for i in loss[0]['labels'][:]:\n",
        "    print(labelsl[i])\n",
        "    print('-----------------')\n",
        "\n",
        "print('the real labels ')\n",
        "for i in images_classe:\n",
        "    print(i)\n",
        "    print('-----------------')\n"
      ],
      "metadata": {
        "id": "F7Fjm_2_gRza"
      },
      "id": "F7Fjm_2_gRza",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Google_Imagenet_load_images.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}